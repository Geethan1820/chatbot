Intelligent Assistant using LLM

---

````markdown
# üß† ProChat ‚Äì Local AI Chatbot App

ProChat is a powerful local AI chatbot built using **Flask** that:
- Accepts user input and files (PDF, TXT, CSV)
- Streams responses from a **local Mistral model (Ollama)**
- Uses **web search fallback** for 2024‚Äì2025 queries (via Wikipedia)
- Performs **data analytics** from uploaded CSVs
- Renders formatted answers and code blocks (like ChatGPT)

---

## üöÄ Features

- ‚úÖ Chat interface with typing animation and markdown support
- üìé PDF & TXT file upload with automatic content extraction
- üåê Web fallback using Wikipedia when local model is unsure
- üìä Data Analytics tool to visualize bar, pie, and line charts from uploaded CSVs
- üß† Local model support (via Ollama + Mistral)
- üí¨ Greeting responses to "hi", "hello", etc.
- üìù Auto-formatted code blocks with copy buttons

---

## üõ† Requirements

Make sure the following are installed:

```bash
Python 3.10+ (Recommended)
ollama (https://ollama.com)
````

### Python Packages

Install them using:

```bash
pip install -r required.txt
```

**required.txt**:

```
flask
requests
beautifulsoup4
PyPDF2
pandas
matplotlib
werkzeug
```

---

## üí° How to Run the App

1. **Install Ollama + Mistral locally**:

   ```bash
   ollama pull mistral
   ```

2. **Run the Flask app**:

   ```bash
   python app.py
   ```

3. **Visit in browser**:

   ```
   http://127.0.0.1:5000/
   ```

---

## üìÅ Project Structure

```
ProChat/
‚îÇ
‚îú‚îÄ‚îÄ app.py                # Main Flask backend
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ chat.html         # Frontend chat UI
‚îú‚îÄ‚îÄ uploads/              # Uploaded PDF, TXT, CSV files
‚îú‚îÄ‚îÄ required.txt          # Python dependencies
‚îú‚îÄ‚îÄ README.md             # This file
```

---

## üåê Web Fallback Examples

For future-based or trending queries, the app fetches real-time info via Wikipedia (2024‚Äì2025):

> **User:** Which team won the IPL 2025 title?
> **Response:** Royal Challengers Bengaluru (RCB) won the title by defeating PBKS by 6 runs...

---

## üìä Data Analytics

Click **"Tools ‚Üí Data Analytics"**, upload a CSV, and choose:

* `1` for Pie Chart
* `2` for Bar Chart
* `3` for Line Chart

Downloadable image of the chart appears inside the chat interface.

---

## ü§ñ Local Model via Ollama

Ollama handles the local LLM (like Mistral). Make sure it‚Äôs running on port `11434`.

```bash
ollama run mistral
```

---

## ü§ù Credits

* [Ollama](https://ollama.com) for local model hosting
* [Mistral](https://mistral.ai) for open weights
* [Wikipedia REST API](https://en.wikipedia.org/api/rest_v1/) for web fallback
* Built with ‚ù§Ô∏è using Flask and Open Source tools

---

## üìú License

This project is free and open-source under the [MIT License](LICENSE)


AUTHOR
VIJAY CHIDAMBARANATHAN - GEETHAN 
---

```

Let me know if you'd also like me to generate:
- A matching `LICENSE` file
- Screenshots for your `README`
- GitHub repository setup instructions
```
